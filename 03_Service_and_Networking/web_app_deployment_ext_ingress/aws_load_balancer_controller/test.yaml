apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/restartedAt: "2025-11-20T17:45:55Z"
    prometheus.io/port: "8080"
    prometheus.io/scrape: "true"
  creationTimestamp: "2025-11-20T17:47:36Z"
  generateName: aws-load-balancer-controller-65fd4d67f9-
  generation: 1
  labels:
    app.kubernetes.io/instance: aws-load-balancer-controller
    app.kubernetes.io/name: aws-load-balancer-controller
    pod-template-hash: 65fd4d67f9
  name: aws-load-balancer-controller-65fd4d67f9-8v7rg
  namespace: kube-system
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: aws-load-balancer-controller-65fd4d67f9
    uid: 2797da2e-7349-4672-bec6-d478d910c0a0
  resourceVersion: "275462"
  uid: 4d793414-6306-4dac-aa72-4ddc8df5417f
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - aws-load-balancer-controller
          topologyKey: kubernetes.io/hostname
        weight: 100
  containers:
  - args:
    - --cluster-name=kubernetes
    - --ingress-class=alb
    image: public.ecr.aws/eks/aws-load-balancer-controller:v2.15.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 2
      httpGet:
        path: /healthz
        port: 61779
        scheme: HTTP
      initialDelaySeconds: 30
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 10
    name: aws-load-balancer-controller
    ports:
    - containerPort: 9443
      name: webhook-server
      protocol: TCP
    - containerPort: 8080
      name: metrics-server
      protocol: TCP
    readinessProbe:
      failureThreshold: 2
      httpGet:
        path: /readyz
        port: 61779
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 10
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      runAsNonRoot: true
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /tmp/k8s-webhook-server/serving-certs
      name: cert
      readOnly: true
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-2gbb4
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""
  preemptionPolicy: PreemptLowerPriority
  priority: 2000000000
  priorityClassName: system-cluster-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    fsGroup: 65534
  serviceAccount: aws-load-balancer-controller
  serviceAccountName: aws-load-balancer-controller
  terminationGracePeriodSeconds: 10
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/network-unavailable
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: cert
    secret:
      defaultMode: 420
      secretName: aws-load-balancer-tls
  - name: kube-api-access-2gbb4
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2025-11-20T17:47:36Z"
    message: '0/3 nodes are available: 1 node(s) didn''t match Pod''s node affinity/selector,
      1 node(s) had untolerated taint {c1-node2: true}, 1 node(s) had untolerated
      taint {node.kubernetes.io/disk-pressure: }. preemption: 0/3 nodes are available:
      3 Preemption is not helpful for scheduling.'
    reason: Unschedulable
    status: "False"
    type: PodScheduled
  phase: Pending
  qosClass: BestEffort